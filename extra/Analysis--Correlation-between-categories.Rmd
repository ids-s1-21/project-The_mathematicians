---
title: "Analysis: Correlations between answers"
author: "The Mathematicians"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "/cloud/project/data")
```

## Libraries

```{r libraries}
library(tidyverse)
library(broom)
library(dplyr)
library(patchwork)
library(tidymodels)
```

## Load the necesary data

```{r load-data}
responses_numeric1_filtered <- read_csv("responses_numeric1_filtered.csv")
```

## Group the data based on their categories

In order to analyse the correlation between answers to questions in different categories, we need to firstly group the data based on categories. And during this section of analysis, we are going to use the data set where scores are given binominally (either 1 or 0) for each individual question. Also, we still need to discard those who didn't finish the whole survey and just completed a part of the questions (as they are less likely to do the survey seriously).  

```{r discard-and-group-data}
not_finish_list <- responses_numeric1_filtered %>%
  filter(is.na(answer) == TRUE) 
not_finish_list <- as_tibble(unique(not_finish_list$anon_id))
not_finish_list <- not_finish_list %>%
  rename(anon_id = value)

responses_part2 <- anti_join(responses_numeric1_filtered, not_finish_list, by = "anon_id")

responses_summary_part2 <- responses_part2 %>%
  filter(qnum != 19) %>%
  group_by(anon_id, category) %>%
  summarise(sum_category_mark = sum(mark))

responses_summary_part2 <- responses_summary_part2 %>%
  pivot_wider(names_from = category, values_from = sum_category_mark)

```
## Pearson correlation test

  Based on the result we obtained above, we could test the correlations between different marks for different categories. We are going to use pearson coefficient which can accurately show their level of correlation. For those with relative high levels of correlation, we could do deeper investigations.  
  We are going to make a matrix where the intercepted entry between each row and column represents the correlation between the variables (category) represented by each row and column (Each row and column will represent a category)

```{r pearson-correlations-table}
correlation_matrix <- data.frame(matrix(ncol = 8, nrow = 8))
rownames(correlation_matrix) <- colnames(responses_summary_part2[2:9])
colnames(correlation_matrix) <- colnames(responses_summary_part2[2:9])
for (i in 1:8) {
  for (j in 1:8) {
    x <- responses_summary_part2[i + 1]
    y <- responses_summary_part2[j + 1]
    correlation_matrix[i, j] <- cor(x, y, method = "pearson")
  }
}

correlation_matrix <- correlation_matrix %>%
  select(-c("no_category"))
correlation_matrix <- correlation_matrix[-c(5),]

print(correlation_matrix)
```

  It seems that these categories don't have significant linear correlations. Therefore, we may need to fit them to a model other than a simple linear model. And, after some processings, we can change the value of the column we need to use to be binomial and then we can apply logistic regression model.  
  
  We know that for each individual question, the mark is either 0 or 1 and each category contains several questions like this. Therefore, the distribution of total mark of each category is similar to a binomial distribution. The difference is that in binomial distribution, the probability of getting each question correctly is fixed while in this situation the probability may not be fixed.  
  
  However, we know that if a student answers each question by pure guessing, the probability of answering each question correctly will just be 2/5 (Since "Neutral" will always get a score of 0). Therefore, we could calculate a "critical number" such that if a student gets a number of correct answers that is less than the critical number, than the probability that the student is guessing the answer is more than the probability that the student is doing these questions seriously.  
  
  Based on this, we could turn the total mark of each student into 1 or 0 according to the following rule:  If the number of correct answer is smaller than the "critical number", we can mark them as 0, otherwise 1.
  
  When it comes to the calculation of critical number, we could simply find a number "m" for each category such that the probability for the number of correct answers to be less than "m" is greater than 50% under the distribution of Bin(n, 1/2) where n is the total number of question in each category.  
  
  For example, if there are 4 question in a category, we can find that P(Right answer <= 2) = (1/2)^4 * (4C0 + 4C1 + 4C2) = 0.6875 while P(Right answer <= 1) < 0.5. In this case, the "critical number" will be 2.   
  
  According to the method of categorising questions described in the previous section, we can get the following critical number for each category:  
  
  - Confidence in Mathematics: 2
  
  - Persistence in Problem Solving: 2 
  
  - Growth Mindset: 2 
  
  - Interest in Mathematics: 1
  
  - Relationship between Mathematics and Real World: 1 
  
  - Sense Making: 2  
  
  - Nature of the Answers: 3  
  
First we are going to test our model on "interest" and "real_world", where we decided to let "real_world" to be our outcome.

```{r change-to-(0,1)}
interest_realworld_data <- responses_summary_part2 %>%
  mutate(
    interest = as_factor(interest),
    real_world = as_factor(if_else(real_world <= 1, 0 ,1)),
  ) %>%
  select(interest, real_world)
```
Before creating a recipe, we first need to split the data into training data and testing data.

```{r split-data}
set.seed(9841)
data_split <- initial_split(interest_realworld_data, prop = 0.8)
training_data = training(data_split)
testing_data = testing(data_split)
```


Then we start to create a data recipe and a corresponding model.

```{r create-recipe}
data_rec <- recipe(
  real_world ~ interest,
  data = training_data
) %>%
  step_dummy(all_nominal(), -all_outcomes())

data_model <- logistic_reg() %>%
  set_engine("glm")
```

After that, we make a workflow and fit the data to the model.

```{r make-workflow}
data_workflow <- workflow() %>%
  add_model(data_model) %>%
  add_recipe(data_rec)

data_fit <- data_workflow %>%
  fit(data = training_data)

tidy(data_fit)
```

Then we are going to use this model to predict the outcome of testing data in order to see whether the model fit is good.

```{r test-predict}
data_predict <- predict(data_fit, testing_data, type = "prob") %>%
  bind_cols(testing_data)
data_predict %>%
  select(.pred_0, .pred_1, real_world)
```

Also, we can make an roc curve for our model.

```{r roc-curve}
data_predict %>%
  roc_auc(
    truth = real_world,
    .pred_1,
    event_level = "second"
  )

```

## Writing the model-fitting function (Not finished)

As there are multiple groups of values that is needed to be fitted, we decided to create a function in order to prevent writing codes repetitively,

```{r function-creating}
model_fitting <- function(responses_binary, dep_var, indep_var) {
  
  
  for (i in 1:8) {
    if (colnames(responses_binary)[i] == dep_var) {
      colnames(responses_binary)[i] <- "dep_var"
      }
  }  
  for (j in 1:8) {
    if (colnames(responses_binary)[j] == indep_var) {
      colnames(responses_binary)[j] <- "indep_var"
      }
  }

  #Split the data to training data and testing data
  set.seed(9841)
  data_split <- initial_split(responses_binary, prop = 0.8)
  training_data = training(data_split)
  testing_data = testing(data_split)
    
  
  #Create a recipe
  data_rec <- recipe(
    dep_var ~ indep_var,
    data = training_data
  ) %>%
    step_dummy(all_nominal(), -all_outcomes())
  data_model <- logistic_reg() %>%
    set_engine("glm")
  
  #Create workflow
  data_workflow <- workflow() %>%
    add_model(data_model) %>%
    add_recipe(data_rec)
  data_fit <- data_workflow %>%
    fit(data = training_data)
  
  #Show the model (to be considered)
  tidy(data_fit)
  return(data_fit)
}
```

## Creating AUC matrix

We need to create an AUC matrix in order to find the best model (those with the highest roc).

```{r roc-matrix}


```

