---
title: "Analysis: Correlations between answers"
author: "The Mathematicians"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "/cloud/project/data")
```

## Libraries

```{r libraries}
library(tidyverse)
library(broom)
library(dplyr)
library(patchwork)
library(tidymodels)
library(gtools)
```

## Load the necesary data

```{r load-data}
responses_numeric1_filtered <- read_csv("responses_numeric1_filtered.csv")
```

## Group the data based on their categories

In order to analyse the correlation between answers to questions in different categories, we need to firstly group the data based on categories. And during this section of analysis, we are going to use the data set where scores are given binominally (either 1 or 0) for each individual question. Also, we still need to discard those who didn't finish the whole survey and just completed a part of the questions (as they are less likely to do the survey seriously).  

```{r discard-and-group-data}
not_finish_list <- responses_numeric1_filtered %>%
  filter(is.na(answer) == TRUE) 
not_finish_list <- as_tibble(unique(not_finish_list$anon_id))
not_finish_list <- not_finish_list %>%
  rename(anon_id = value)

responses_part2 <- anti_join(responses_numeric1_filtered, not_finish_list, by = "anon_id")

responses_summary_part2 <- responses_part2 %>%
  filter(qnum != 19) %>%
  group_by(anon_id, category) %>%
  summarise(sum_category_mark = sum(mark))

responses_summary_part2 <- responses_summary_part2 %>%
  pivot_wider(names_from = category, values_from = sum_category_mark)

```
## Pearson correlation test

  Based on the result we obtained above, we could test the correlations between different marks for different categories. We are going to use pearson coefficient which can accurately show their level of correlation. For those with relative high levels of correlation, we could do deeper investigations.  
  We are going to make a matrix where the intercepted entry between each row and column represents the correlation between the variables (category) represented by each row and column (Each row and column will represent a category)

```{r pearson-correlations-table}
correlation_matrix <- data.frame(matrix(ncol = 8, nrow = 8))
rownames(correlation_matrix) <- colnames(responses_summary_part2[2:9])
colnames(correlation_matrix) <- colnames(responses_summary_part2[2:9])
for (i in 1:8) {
  for (j in 1:8) {
    x <- responses_summary_part2[i + 1]
    y <- responses_summary_part2[j + 1]
    correlation_matrix[i, j] <- cor(x, y, method = "pearson")
  }
}

correlation_matrix <- correlation_matrix %>%
  select(-c("no_category"))
correlation_matrix <- correlation_matrix[-c(5),]

print(correlation_matrix)
```

  It seems that these categories don't have significant linear correlations. Therefore, we may need to fit them to a model other than a simple linear model. And, after some processings, we can change the value of the column we need to use to be binomial and then we can apply logistic regression model.  
  
  We know that for each individual question, the mark is either 0 or 1 and each category contains several questions like this. Therefore, the distribution of total mark of each category is similar to a binomial distribution. The difference is that in binomial distribution, the probability of getting each question correctly is fixed while in this situation the probability may not be fixed.  
  
  However, we know that if a student answers each question by pure guessing, the probability of answering each question correctly will just be 2/5 (Since "Neutral" will always get a score of 0). Therefore, we could calculate a "critical number" such that if a student gets a number of correct answers that is less than the critical number, than the probability that the student is guessing the answer is more than the probability that the student is doing these questions seriously.  
  
  Based on this, we could turn the total mark of each student into 1 or 0 according to the following rule:  If the number of correct answer is smaller than the "critical number", we can mark them as 0, otherwise 1.
  
  When it comes to the calculation of critical number, we could simply find a number "m" for each category such that the probability for the number of correct answers to be less than "m" is greater than 50% under the distribution of Bin(n, 1/2) where n is the total number of question in each category.  
  
  For example, if there are 4 question in a category, we can find that P(Right answer <= 2) = (1/2)^4 * (4C0 + 4C1 + 4C2) = 0.6875 while P(Right answer <= 1) < 0.5. In this case, the "critical number" will be 2.   
  
  According to the method of categorising questions described in the previous section, we can get the following critical number for each category:  
  
  - Confidence in Mathematics: 2
  
  - Persistence in Problem Solving: 2 
  
  - Growth Mindset: 2 
  
  - Interest in Mathematics: 1
  
  - Relationship between Mathematics and Real World: 1 
  
  - Sense Making: 2  
  
  - Nature of the Answers: 3  
  
## Writing the model-fitting function

As there are multiple groups of values that is needed to be fitted, we decided to create a function in order to prevent writing codes repetitively.

```{r function-creating}

model_fitting <- function(responses_binary) {

  #Split the data to training data and testing data
  set.seed(9841)
  data_split <- initial_split(responses_binary, prop = 0.8)
  training_data = training(data_split)
  testing_data = testing(data_split)
    
  
  #Create a recipe
  data_rec <- recipe(
    dep_var ~ indep_var,
    data = training_data
  ) %>%
    step_dummy(all_nominal(), -all_outcomes())
  data_model <- logistic_reg() %>%
    set_engine("glm")
  
  #Create workflow
  data_workflow <- workflow() %>%
    add_model(data_model) %>%
    add_recipe(data_rec)
  data_fit <- data_workflow %>%
    fit(data = training_data)
  
  data_predict <- predict(data_fit, testing_data, type = "prob") %>%
    bind_cols(testing_data)
  
  return(data_predict)
}
```

## Creating ROC matrix

We need to create an ROC matrix in order to find the models that are worth paying attention by comparing their roc.

```{r roc-matrix}
roc_matrix <- data.frame(matrix(ncol = 8, nrow = 8))
rownames(roc_matrix) <- colnames(responses_summary_part2[2:9])
colnames(roc_matrix) <- colnames(responses_summary_part2[2:9])

roc_matrix <- roc_matrix %>%
  select(-c("no_category"))
roc_matrix <- roc_matrix[-c(5),]

variable_matrix <- permutations(
  8, 
  2, 
  v = colnames(responses_summary_part2)[2:9], 
  repeats.allowed = FALSE
  ) %>%
  as_tibble() %>%
  filter(V1 != "no_category" & V2 != "no_category") %>%
  as.matrix() %>%
  t() 

for (i in 1:42) {
  variable_col <- variable_matrix[i]
  responses_binary <- responses_summary_part2 %>%
    ungroup() %>%
    select(all_of(c(variable_matrix[1,i], variable_matrix[2,i]))) %>%
    mutate(
      dep_var = case_when(
        as.logical(variable_matrix[1,i] == "confidence") ~ if_else(V1 <= 2, 0, 1),
        as.logical(variable_matrix[1,i] == "growth_mindset") ~ if_else(V1 <= 2, 0, 1),
        as.logical(variable_matrix[1,i] == "interest") ~ if_else(V1 <= 1, 0, 1),
        as.logical(variable_matrix[1,i] == "nature") ~ if_else(V1 <= 3, 0, 1),
        as.logical(variable_matrix[1,i] == "persistence") ~ if_else(V1 <= 2, 0, 1),
        as.logical(variable_matrix[1,i] == "real_world") ~ if_else(V1 <= 1, 0, 1),
        as.logical(variable_matrix[1,i] == "sense") ~ if_else(V1 <= 2, 0, 1)
      )
    ) %>%
    mutate(dep_var = as_factor(dep_var)) %>%
    mutate(indep_var = as_factor(V2)) 
  roc_value <- model_fitting(responses_binary) %>%
  roc_auc(
    truth = dep_var,
    .pred_1,
    event_level = "second"
  )
  roc_matrix[variable_matrix[1,i], variable_matrix[2,i]] <- roc_value$.estimate
}


```

## Find models that are worth paying attention

First, let's take a look at the ROC matrix. The rows of the matrix represent dependent variables and rows represent independent variables.

```{r show-matrix}
print(roc_matrix)
```
  We can see that the area under the ROC curve when interest is the dependent variable and confidence is the independent variable is 0.8321314, which is the highest in the matrix. So we will take a look at the model.
  
```{r interest-confidence-model}

responses_interest_confidence <- responses_summary_part2 %>%
  ungroup() %>%
  select(interest, confidence) %>%
  mutate(
    interest = as_factor(if_else(interest <= 1, 0, 1)),
    confidence = as_factor(confidence)
  ) 

set.seed(9841)
interest_confidence_split <- initial_split(responses_interest_confidence, prop = 0.8)
interest_confidence_training_data = training(interest_confidence_split)
interest_confidence_testing_data = testing(interest_confidence_split)
interest_confidence_rec <- recipe(
  interest ~ confidence,
  data = interest_confidence_training_data
) %>%
  step_dummy(all_nominal(), -all_outcomes())
interest_confidence_model <- logistic_reg() %>%
  set_engine("glm")
interest_confidence_workflow <- workflow() %>%
  add_model(interest_confidence_model) %>%
  add_recipe(interest_confidence_rec)
interest_confidence_fit <- interest_confidence_workflow %>%
  fit(data = interest_confidence_training_data)
tidy(interest_confidence_fit)
```
  
  <Interpretation>
  
  Also, we could show its ROC to see how the sensitivity and specificity compares.

```{r interest-confidence-roc}
predict(interest_confidence_fit, interest_confidence_testing_data, type = "prob") %>%
  bind_cols(interest_confidence_testing_data) %>%
  roc_curve(
    truth = interest,
    .pred_1,
    event_level = "second"
  ) %>%
  autoplot()

```

  In addition to these two variables, "interest" (dependent variable) and "sense" (independent variable) also shows a large area under the ROC. We will also take a look at their model.
  
```{r interest-sense-model}
responses_interest_sense <- responses_summary_part2 %>%
  ungroup() %>%
  select(interest, sense) %>%
  mutate(
    interest = as_factor(if_else(interest <= 1, 0, 1)),
    sense = as_factor(sense)
  ) 

set.seed(9841)
interest_sense_split <- initial_split(responses_interest_sense, prop = 0.8)
interest_sense_training_data = training(interest_sense_split)
interest_sense_testing_data = testing(interest_sense_split)
interest_sense_rec <- recipe(
  interest ~ sense,
  data = interest_sense_training_data
) %>%
  step_dummy(all_nominal(), -all_outcomes())
interest_sense_model <- logistic_reg() %>%
  set_engine("glm")
interest_sense_workflow <- workflow() %>%
  add_model(interest_sense_model) %>%
  add_recipe(interest_sense_rec)
interest_sense_fit <- interest_sense_workflow %>%
  fit(data = interest_sense_training_data)
tidy(interest_sense_fit)
```

 <Interpretation>
 
 Also, we could show its ROC to see how the sensitivity and specificity compares.

```{r interest-sense-roc}
predict(interest_sense_fit, interest_sense_testing_data, type = "prob") %>%
  bind_cols(interest_sense_testing_data) %>%
  roc_curve(
    truth = interest,
    .pred_1,
    event_level = "second"
  ) %>%
  autoplot()


```

